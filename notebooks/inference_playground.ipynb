{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OqEP1SlGeVZ"
      },
      "source": [
        "# SAM: Inference Playground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dE2hzjSNQs0p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content')\n",
        "CODE_DIR = 'SAM'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbaMZ40hQxT0",
        "outputId": "ca2e836d-a197-4830-ecd7-e8c91ed4e138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'SAM' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/yuval-alaluf/SAM.git $CODE_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43F-3KfeQ08S",
        "outputId": "3851812c-832b-40cd-81c5-2710267f9c43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-13 03:29:33--  https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241213%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241213T032933Z&X-Amz-Expires=300&X-Amz-Signature=cc307aa42e1fa5bd9461c2254e2a82978b4affa96d8ef9b20901c755e41da553&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-12-13 03:29:33--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241213%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241213T032933Z&X-Amz-Expires=300&X-Amz-Signature=cc307aa42e1fa5bd9461c2254e2a82978b4affa96d8ef9b20901c755e41da553&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77854 (76K) [application/octet-stream]\n",
            "Saving to: ‘ninja-linux.zip.1’\n",
            "\n",
            "ninja-linux.zip.1   100%[===================>]  76.03K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-12-13 03:29:33 (5.01 MB/s) - ‘ninja-linux.zip.1’ saved [77854/77854]\n",
            "\n",
            "Archive:  ninja-linux.zip\n",
            "replace /usr/local/bin/ninja? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: /usr/local/bin/ninja    \n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "av0207x4Q2iL"
      },
      "outputs": [],
      "source": [
        "os.chdir(f'./{CODE_DIR}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Zvwx9NsiQq9t"
      },
      "outputs": [],
      "source": [
        "from argparse import Namespace\n",
        "import os\n",
        "import sys\n",
        "import pprint\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "from datasets.augmentations import AgeTransformer\n",
        "from utils.common import tensor2im\n",
        "from models.psp import pSp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "uj3dJjQsQq9y"
      },
      "outputs": [],
      "source": [
        "EXPERIMENT_TYPE = 'ffhq_aging'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mStxrAtuQq9y"
      },
      "source": [
        "## Step 1: Download Pretrained Model\n",
        "As part of this repository, we provide our pretrained aging model.\n",
        "We'll download the model for the selected experiments as save it to the folder `../pretrained_models`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_pC38oLGQq9z"
      },
      "outputs": [],
      "source": [
        "def get_download_model_command(file_id, file_name):\n",
        "    \"\"\" Get wget download command for downloading the desired model and save to directory ../pretrained_models. \"\"\"\n",
        "    current_directory = os.getcwd()\n",
        "    save_path = os.path.join(os.path.dirname(current_directory), \"pretrained_models\")\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "    url = r\"\"\"wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id={FILE_ID}' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id={FILE_ID}\" -O {SAVE_PATH}/{FILE_NAME} && rm -rf /tmp/cookies.txt\"\"\".format(FILE_ID=file_id, FILE_NAME=file_name, SAVE_PATH=save_path)\n",
        "    return url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "rOQ2Vz2kQq9z"
      },
      "outputs": [],
      "source": [
        "MODEL_PATHS = {\n",
        "    \"ffhq_aging\": {\"id\": \"1XyumF6_fdAxFmxpFcmPf-q84LU_22EMC\", \"name\": \"sam_ffhq_aging.pt\"}\n",
        "}\n",
        "\n",
        "path = MODEL_PATHS[EXPERIMENT_TYPE]\n",
        "download_command = get_download_model_command(file_id=path[\"id\"], file_name=path[\"name\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0nHPvo5Qq9z",
        "outputId": "245abf29-de26-42ba-dfaa-f4a22cd9efa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-13 03:29:45--  http://wget/\n",
            "Resolving wget (wget)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘wget’\n",
            "--2024-12-13 03:29:45--  https://docs.google.com/uc?export=download&confirm=&id=1XyumF6_fdAxFmxpFcmPf-q84LU_22EMC\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.98.138, 108.177.98.100, 108.177.98.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.98.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1XyumF6_fdAxFmxpFcmPf-q84LU_22EMC&export=download [following]\n",
            "--2024-12-13 03:29:45--  https://drive.usercontent.google.com/download?id=1XyumF6_fdAxFmxpFcmPf-q84LU_22EMC&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.197.132, 2607:f8b0:400e:c07::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.197.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2429 (2.4K) [text/html]\n",
            "Saving to: ‘/content/pretrained_models/sam_ffhq_aging.pt’\n",
            "\n",
            "/content/pretrained 100%[===================>]   2.37K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-13 03:29:45 (42.2 MB/s) - ‘/content/pretrained_models/sam_ffhq_aging.pt’ saved [2429/2429]\n",
            "\n",
            "FINISHED --2024-12-13 03:29:45--\n",
            "Total wall clock time: 0.3s\n",
            "Downloaded: 1 files, 2.4K in 0s (42.2 MB/s)\n"
          ]
        }
      ],
      "source": [
        "!wget {download_command}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvRDiRrMQq90"
      },
      "source": [
        "## Step 2: Define Inference Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNaSSzZsQq90"
      },
      "source": [
        "Below we have a dictionary defining parameters such as the path to the pretrained model to use and the path to the\n",
        "image to perform inference on.\n",
        "While we provide default values to run this script, feel free to change as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "yaGqalwuQq90"
      },
      "outputs": [],
      "source": [
        "EXPERIMENT_DATA_ARGS = {\n",
        "    \"ffhq_aging\": {\n",
        "        \"model_path\": \"/content/pretrained_models/sam_ffhq_aging.pt\",\n",
        "        \"image_path\": \"notebooks/images/866.jpg\",\n",
        "        \"transform\": transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "wjkLqLkDQq90"
      },
      "outputs": [],
      "source": [
        "EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS[EXPERIMENT_TYPE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkfqoKJwQq91"
      },
      "source": [
        "## Step 3: Load Pretrained Model\n",
        "We assume that you have downloaded the pretrained aging model and placed it in the path defined above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "cZuho98JQq91",
        "outputId": "d5152131-9552-4de4-df7d-516bb16b625c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-1b9ef214fde3>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(model_path, map_location='cpu')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "invalid load key, '<'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-1b9ef214fde3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEXPERIMENT_ARGS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1382\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m         return _legacy_load(\n\u001b[0m\u001b[1;32m   1385\u001b[0m             \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1626\u001b[0m         )\n\u001b[1;32m   1627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1628\u001b[0;31m     \u001b[0mmagic_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1629\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmagic_number\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mMAGIC_NUMBER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid magic number; corrupt file?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '<'."
          ]
        }
      ],
      "source": [
        "model_path = EXPERIMENT_ARGS['model_path']\n",
        "ckpt = torch.load(model_path, map_location='cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "f6NOxONxQq91",
        "outputId": "c9301f75-5e85-4eba-8e2c-e10fe8ef270e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ckpt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-b95cf17c9f21>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'opts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ckpt' is not defined"
          ]
        }
      ],
      "source": [
        "opts = ckpt['opts']\n",
        "pprint.pprint(opts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6c93qE9Qq91"
      },
      "outputs": [],
      "source": [
        "# update the training options\n",
        "opts['checkpoint_path'] = model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRTfKFrkQq91"
      },
      "outputs": [],
      "source": [
        "opts = Namespace(**opts)\n",
        "net = pSp(opts)\n",
        "net.eval()\n",
        "net.cuda()\n",
        "print('Model successfully loaded!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6BegCirQq92"
      },
      "source": [
        "## Step 4: Visualize Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kc4Sr31TQq92"
      },
      "outputs": [],
      "source": [
        "image_path = EXPERIMENT_DATA_ARGS[EXPERIMENT_TYPE][\"image_path\"]\n",
        "original_image = Image.open(image_path).convert(\"RGB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKA9BO9_Qq92"
      },
      "outputs": [],
      "source": [
        "original_image.resize((256, 256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3a50tAcQq92"
      },
      "source": [
        "## Step 5: Perform Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6oqf8JwzK0K"
      },
      "source": [
        "### Align Image\n",
        "\n",
        "Before running inference we'll run alignment on the input image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y244_ejy9Drx"
      },
      "outputs": [],
      "source": [
        "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ9Ce1aYzmFF"
      },
      "outputs": [],
      "source": [
        "def run_alignment(image_path):\n",
        "    import dlib\n",
        "    from scripts.align_all_parallel import align_face\n",
        "    predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "    aligned_image = align_face(filepath=image_path, predictor=predictor)\n",
        "    print(\"Aligned image has shape: {}\".format(aligned_image.size))\n",
        "    return aligned_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTZcKMdK8y77"
      },
      "outputs": [],
      "source": [
        "aligned_image = run_alignment(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUBAfodh5PaM"
      },
      "outputs": [],
      "source": [
        "aligned_image.resize((256, 256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMyoh4X1HYAS"
      },
      "source": [
        "### Run Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkzQpi1aQq92"
      },
      "outputs": [],
      "source": [
        "img_transforms = EXPERIMENT_ARGS['transform']\n",
        "input_image = img_transforms(aligned_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lI7yWNPDQq92"
      },
      "outputs": [],
      "source": [
        "# we'll run the image on multiple target ages\n",
        "target_ages = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "age_transformers = [AgeTransformer(target_age=age) for age in target_ages]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLP4pF-2Qq93"
      },
      "outputs": [],
      "source": [
        "def run_on_batch(inputs, net):\n",
        "    result_batch = net(inputs.to(\"cuda\").float(), randomize_noise=False, resize=False)\n",
        "    return result_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfrY_gEEQq93"
      },
      "outputs": [],
      "source": [
        "# for each age transformed age, we'll concatenate the results to display them side-by-side\n",
        "results = np.array(aligned_image.resize((1024, 1024)))\n",
        "for age_transformer in age_transformers:\n",
        "    print(f\"Running on target age: {age_transformer.target_age}\")\n",
        "    with torch.no_grad():\n",
        "        input_image_age = [age_transformer(input_image.cpu()).to('cuda')]\n",
        "        input_image_age = torch.stack(input_image_age)\n",
        "        result_tensor = run_on_batch(input_image_age, net)[0]\n",
        "        result_image = tensor2im(result_tensor)\n",
        "        results = np.concatenate([results, result_image], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFgwfLTKQq93"
      },
      "source": [
        "### Visualize Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpwyrv0iQq93"
      },
      "outputs": [],
      "source": [
        "results = Image.fromarray(results)\n",
        "results   # this is a very large image (11*1024 x 1024) so it may take some time to display!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sL7fHp9Qq93",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# save image at full resolution\n",
        "results.save(\"notebooks/images/age_transformed_image.jpg\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "inference_playground.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}